{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faceDetection\n",
    "import re\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#https://www.geeksforgeeks.org/face-recognition-with-local-binary-patterns-lbps-and-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_labels(labels):\n",
    "    import json\n",
    "    # Specify the file path\n",
    "    file_path = 'labels_ids.json'\n",
    "\n",
    "    # Save dictionary to a file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(labels, file)\n",
    "\n",
    "\n",
    "def load_labels():\n",
    "\n",
    "    import json\n",
    "\n",
    "    # Specify the file path\n",
    "    file_path = 'labels_ids.json'\n",
    "\n",
    "    # Load dictionary from file\n",
    "    with open(file_path, 'r') as file:\n",
    "        loaded_dict = json.load(file)\n",
    "\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pedro': 1, 'Carl': 2, 'Jessica': 3, 'Amanda': 4, 'Albert Mazuz': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=load_labels()\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Mazuz (0).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (1).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (10).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (11).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (12).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (13).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (14).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (15).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (16).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (17).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (18).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (19).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (2).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (20).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (21).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (22).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (23).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (3).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (4).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (5).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (6).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (7).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (8).jpg\n",
      "(480, 640, 3)\n",
      "Albert Mazuz (9).jpg\n",
      "(480, 640, 3)\n",
      "Amanda (1).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (10).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (11).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (12).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (13).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (14).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (15).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (16).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (17).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (18).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (19).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (2).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (20).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (3).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (4).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (5).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (6).jpg\n",
      "(592, 896, 3)\n",
      "Imagem com mais de uma face. Rejeitada\n",
      "Amanda (7).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (8).jpg\n",
      "(592, 896, 3)\n",
      "Amanda (9).jpg\n",
      "(592, 896, 3)\n",
      "Carl (1).jpg\n",
      "(592, 896, 3)\n",
      "Carl (10).jpg\n",
      "(592, 896, 3)\n",
      "Carl (11).jpg\n",
      "(592, 896, 3)\n",
      "Carl (12).jpg\n",
      "(592, 896, 3)\n",
      "Carl (13).jpg\n",
      "(592, 896, 3)\n",
      "Carl (14).jpg\n",
      "(592, 896, 3)\n",
      "Carl (15).jpg\n",
      "(592, 896, 3)\n",
      "Carl (16).jpg\n",
      "(592, 896, 3)\n",
      "Carl (17).jpg\n",
      "(592, 896, 3)\n",
      "Carl (18).jpg\n",
      "(592, 896, 3)\n",
      "Carl (19).jpg\n",
      "(592, 896, 3)\n",
      "Carl (2).jpg\n",
      "(592, 896, 3)\n",
      "Carl (20).jpg\n",
      "(592, 896, 3)\n",
      "Carl (3).jpg\n",
      "(592, 896, 3)\n",
      "Carl (4).jpg\n",
      "(592, 896, 3)\n",
      "Carl (5).jpg\n",
      "(592, 896, 3)\n",
      "Carl (6).jpg\n",
      "(592, 896, 3)\n",
      "Carl (7).jpg\n",
      "(592, 896, 3)\n",
      "Carl (8).jpg\n",
      "(592, 896, 3)\n",
      "Carl (9).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (1).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (10).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (11).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (12).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (13).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (14).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (15).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (16).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (17).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (18).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (19).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (2).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (3).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (4).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (5).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (6).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (7).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (8).jpg\n",
      "(592, 896, 3)\n",
      "Jessica (9).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (1).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (10).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (11).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (12).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (13).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (14).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (15).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (16).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (17).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (18).jpg\n",
      "(592, 896, 3)\n",
      "Imagem com mais de uma face. Rejeitada\n",
      "Pedro (19).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (2).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (3).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (4).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (5).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (6).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (7).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (8).jpg\n",
      "(592, 896, 3)\n",
      "Pedro (9).jpg\n",
      "(592, 896, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "< cv2.face.LBPHFaceRecognizer 00000207C42B7AD0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def train_model(label):\n",
    "    # Create lists to store the face samples and their corresponding labels\n",
    "    faces = []\n",
    "    labels = []\n",
    "    \n",
    "    dir_path='files/caltech_faces/train'\n",
    "    # Load the images from the 'Faces' folder\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        if file_name.endswith('.jpg'):\n",
    "            # Extract the label (person's name) from the file name\n",
    "            name = re.split(r'\\s+\\(.*\\)', file_name)[0]\n",
    "            print(file_name)\n",
    "            # Read the image\n",
    "            image = cv2.imread(os.path.join(dir_path, file_name))\n",
    "\n",
    "            # Crop the detected face region\n",
    "            cropped_faces = faceDetection.alinha_face(image,return_face=True)\n",
    "            # Se existir apenas uma face, salva a imagem e o label\n",
    "            if len(cropped_faces)==1:\n",
    "                # Pega as informações da imagem na lista\n",
    "                face_crop=cropped_faces[0]\n",
    "                # Pega a imagem no dicionario\n",
    "                face_crop=face_crop['image']\n",
    "                # Transforma na escala de cinza\n",
    "                face_crop=cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "                # Append the face sample and label to the lists\n",
    "                faces.append(face_crop)\n",
    "                labels.append(label[name])\n",
    "            else:\n",
    "                print('Imagem com mais de uma face. Rejeitada')\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # Train the face recognition model using the faces and labels\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    # recognizer = cv2.face.LBPHFaceRecognizer_create(radius=2, neighbors=16, grid_x=8, grid_y=8)\n",
    "    start=time.time()\n",
    "    recognizer.train(faces, np.array(labels))\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    recognizer.save('id_recognition_model.xml')\n",
    "    return recognizer\n",
    " \n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "# Train the model\n",
    "Recognizer =train_model(label)\n",
    "Recognizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecendo Usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n",
      "Original name x Recognized Name\n",
      "Albert Mazuz - Albert Mazuz\n",
      "Confidence= 46.82630539815293\n",
      "(800, 800, 3)\n",
      "Original name x Recognized Name\n",
      "Albert Mazuz.jpg - Carl\n",
      "Confidence= 51.05106402358276\n",
      "(592, 896, 3)\n",
      "Original name x Recognized Name\n",
      "Amanda - Amanda\n",
      "Confidence= 25.362745747691907\n",
      "(592, 896, 3)\n",
      "Original name x Recognized Name\n",
      "Carl - Carl\n",
      "Confidence= 20.866773081919167\n",
      "(592, 896, 3)\n",
      "Original name x Recognized Name\n",
      "Jessica - Pedro\n",
      "Confidence= 26.814271081638175\n",
      "(592, 896, 3)\n",
      "Original name x Recognized Name\n",
      "Pedro - Pedro\n",
      "Confidence= 22.597493331669007\n"
     ]
    }
   ],
   "source": [
    "Recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "Recognizer.read('id_recognition_model.xml')\n",
    "\n",
    "\n",
    "\n",
    "def recognize_face(recognizer,label,imagem):\n",
    "    \"\"\"Recognize a cropped image\"\"\"\n",
    "\n",
    "    # Reverse keys and values in the dictionary\n",
    "    label_name = {value: key for key, value in label.items()}\n",
    "    #Preparar Imagem\n",
    "    imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    label,confidence=recognizer.predict(imagem)\n",
    "    return {'name':label_name[label],'confidence':confidence}\n",
    "\n",
    "dir_path='files/caltech_faces/test'\n",
    "\n",
    "for file_name in os.listdir(dir_path):\n",
    "     if file_name.endswith('.jpg'):\n",
    "            image_path=os.path.join(dir_path, file_name)\n",
    "            # Extract the label (person's name) from the file name\n",
    "            name = re.split(r'\\s+\\(.*\\)', file_name)[0]\n",
    "            imagem = cv2.imread(image_path)\n",
    "            # Se número de faces =1\n",
    "            if faceDetection.numero_de_faces(imagem)==1:    \n",
    "                #Crop the image\n",
    "                faces_crop = faceDetection.alinha_face(imagem,return_face=True)\n",
    "                if len(faces_crop)==1:\n",
    "                    face_crop=faces_crop[0]['image']\n",
    "                    #Recognize the person\n",
    "\n",
    "                    recognition=recognize_face(Recognizer,label,face_crop)\n",
    "                    name_recognized=recognition['name']\n",
    "                    confidence=recognition['confidence']\n",
    "                    print(f'Original name x Recognized Name')\n",
    "                    print(f'{name} - {name_recognized}')\n",
    "                    print(f'Confidence= {confidence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturando Imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    " \n",
    "# Generate a face recognition model\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read('id_recognition_model.xml')\n",
    "\n",
    "# Function to capture images and store in dataset folder\n",
    "def capture_images(User):\n",
    "    path='files/caltech_faces/train'\n",
    "    # Create a directory to store the captured images\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs('files/caltech_faces')\n",
    " \n",
    "    # Open the camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    " \n",
    "    # Set the image counter as 0\n",
    "    count = 0\n",
    " \n",
    "    while True:\n",
    "        # Read a frame from the camera\n",
    "        ret, frame = cap.read()\n",
    " \n",
    "        # # Store the captured face images in the Faces folder\n",
    "        # cv2.imwrite(f'{path}/{User} ({count}).jpg', frame)\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        img=frame\n",
    " \n",
    "        # Detect faces in the grayscale frame\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    " \n",
    "        # Draw rectangles around the faces and store the images\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "            # recognize_face(recognizer,label,imagem)\n",
    " \n",
    "\n",
    " \n",
    "            count += 1\n",
    " \n",
    "        # Display the frame with face detection\n",
    "        cv2.imshow('Capture Faces', frame)\n",
    " \n",
    "        # Break the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    " \n",
    "        # Break the loop after capturing a certain number of images\n",
    "        if count >=24:\n",
    "            break\n",
    " \n",
    "    # Release the camera and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "capture_images('Albert Mazuz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando Usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/102 [00:02<04:07,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagem com mais de uma face. Rejeitada\n",
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/102 [00:04<03:38,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagem com mais de uma face. Rejeitada\n",
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/102 [00:06<03:31,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/102 [00:08<03:23,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5/102 [00:10<03:13,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/102 [00:12<03:05,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/102 [00:13<02:58,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/102 [00:15<02:54,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/102 [00:17<02:51,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 10/102 [00:19<02:47,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/102 [00:21<02:44,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/102 [00:22<02:41,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/102 [00:24<02:37,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 14/102 [00:26<02:34,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 15/102 [00:28<02:34,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/102 [00:29<02:34,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/102 [00:31<02:30,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/102 [00:33<02:26,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 19/102 [00:35<02:25,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 20/102 [00:36<02:25,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/102 [00:38<02:26,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/102 [00:40<02:27,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/102 [00:42<02:22,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:44<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "label=load_labels()\n",
    "new_user_name='Albert Mazuz'\n",
    "if new_user_name not in label:\n",
    "    last_label_number=max(labels.values())\n",
    "    label[new_user_name]=last_label_number+1\n",
    "    save_labels(label)\n",
    "\n",
    "label\n",
    "\n",
    "# capture_images(new_user_name)\n",
    "\n",
    "# update_model(new_user_name)\n",
    "    # Create lists to store the face samples and their corresponding labels\n",
    "faces = []\n",
    "labels = []\n",
    "\n",
    "dir_path='files/caltech_faces/train'\n",
    "# Load the images from the 'Faces' folder\n",
    "for file_name in tqdm(os.listdir(dir_path)):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        # Extract the label (person's name) from the file name\n",
    "        name = re.split(r'\\s+\\(.*\\)', file_name)[0]\n",
    "      \n",
    "        if name==new_user_name:\n",
    "            # Read the image\n",
    "            image = cv2.imread(os.path.join(dir_path, file_name))\n",
    "\n",
    "            # print(image.shape)\n",
    "            # Crop the detected face region\n",
    "            cropped_faces = faceDetection.alinha_face(image,return_face=True)\n",
    "            # Se existir apenas uma face, salva a imagem e o label\n",
    "            if len(cropped_faces)==1:\n",
    "                # Pega as informações da imagem na lista\n",
    "                face_crop=cropped_faces[0]\n",
    "                # Pega a imagem no dicionario\n",
    "                face_crop=face_crop['image']\n",
    "                # Transforma na escala de cinza\n",
    "                face_crop=cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "                # Append the face sample and label to the lists\n",
    "                faces.append(face_crop)\n",
    "                labels.append(label[name])\n",
    "            else:\n",
    "                print('Imagem com mais de uma face. Rejeitada') \n",
    "\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read('id_recognition_model.xml')\n",
    "recognizer.update(faces,np.array(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
